# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sT0baao84ohCjpFcRTBCbAptWTTuKpOU
"""

!pip install transformers

!pip install datasets

import json
import re
import torch
from datasets import load_dataset
#from transformers import AutoTokenizer, AutoModelForCausalLM
# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM


tokenizer = AutoTokenizer.from_pretrained("amazon/MistralLite")
model = AutoModelForCausalLM.from_pretrained("amazon/MistralLite").to("cuda")

import json
import re
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

######################################################
# Pomocnicze funkcje do różnych zadań
######################################################

def score_candidate(model, tokenizer, context, candidate):
    text_input = context.strip() + " " + candidate.strip()
    input_ids = tokenizer.encode(text_input, return_tensors="pt").to("cuda")  # Przenieś tensor na GPU
    with torch.no_grad():
        outputs = model(input_ids, labels=input_ids)
    nll = outputs.loss.item() * input_ids.size(1)
    return nll

######################################################
# 1. Test GSM8K
######################################################

def test_gsm8k(model, tokenizer, output_json_path="gsm8k_results.json"):
    dataset = load_dataset("gsm8k", "main")
    test_data = dataset["test"]

    results = []
    correct = 0
    total = len(test_data)

    for i, example in enumerate(test_data):
        question = example["question"]
        reference_answer = extract_number_from_text(example["answer"])

        prompt = f"Question: {question}\nAnswer:"
        input_ids = tokenizer.encode(prompt, return_tensors="pt").to("cuda")  # Przenieś tensor na GPU

        # Generowanie odpowiedzi
        output_ids = model.generate(
            input_ids,
            max_length=256,
            temperature=0.0,
            num_beams=1
        )
        predicted_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)


        predicted_answer = extract_number_from_text(predicted_text)

        is_correct = (predicted_answer == reference_answer)
        if is_correct:
            correct += 1

        results.append({
            "question": question,
            "reference_answer": reference_answer,
            "predicted_text": predicted_text,
            "predicted_answer": predicted_answer,
            "correct": is_correct
        })

        # Logowanie co 100
        if i % 100 == 0:
            print(f"[GSM8K] Przetworzono przykład {i + 1}/{total}.")
            print(f"Pytanie: {question}")
            print(f"Odpowiedź referencyjna: {reference_answer}")
            print(f"Odpowiedź modelu: {predicted_text}")

    accuracy = correct / total if total > 0 else 0.0


    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump({"accuracy": accuracy, "results": results}, f, indent=2, ensure_ascii=False)

    print(f"GSM8K accuracy: {accuracy * 100:.2f}%")
    return accuracy


import re

def extract_number_from_text(text):
    match = re.search(r'\b\d+\b', text)
    return match.group() if match else None

######################################################
# 2. Test ARC (AI2 Reasoning Challenge)
######################################################
def test_arc_challenge(model, tokenizer, output_json_path="arc_results.json"):
    dataset = load_dataset("ai2_arc", "ARC-Challenge")
    test_data = dataset["test"]

    results = []
    correct = 0
    total = 0

    for example in test_data:
        question = example["question"]
        choices = example["choices"]["text"]
        labels = example["choices"]["label"]
        correct_label = example["answerKey"]

        if not choices or not labels or not correct_label:
            continue


        scores = []
        for choice_label, choice_text in zip(labels, choices):

            context = f"Question: {question}\nAnswer:"
            candidate = f" {choice_text}"
            nll = score_candidate(model, tokenizer, context, candidate)
            scores.append((nll, choice_label))

        scores.sort(key=lambda x: x[0])
        predicted_label = scores[0][1]

        is_correct = (predicted_label == correct_label)
        if is_correct:
            correct += 1
        total += 1

        results.append({
            "question": question,
            "choices": dict(zip(labels, choices)),
            "correct_label": correct_label,
            "predicted_label": predicted_label,
            "correct": is_correct
        })

    accuracy = correct / total if total > 0 else 0.0

    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump({
            "accuracy": accuracy,
            "results": results
        }, f, indent=2, ensure_ascii=False)

    return accuracy

######################################################
# 3. Test HellaSwag
######################################################

def test_hellaswag(model, tokenizer, output_json_path="hellaswag_results.json"):
    dataset = load_dataset("hellaswag")
    val_data = dataset["validation"]


    print("Przykładowe dane testowe:")
    for i in range(5):
        print(val_data[i])
    print("--- Koniec podglądu danych testowych ---")

    results = []
    correct = 0
    total = len(val_data)

    for idx, example in enumerate(val_data):
        try:
            context = example.get("ctx", "")
            endings = example.get("endings", []
            correct_label = int(example.get("label", -1))

            if not context or not endings or correct_label == -1 or len(endings) != 4:
                print(f"Pominięto przykład z brakującymi lub niepoprawnymi danymi: {example}")
                continue

            option_scores = []
            for i, candidate in enumerate(endings):
                nll = score_candidate(model, tokenizer, context, candidate)
                option_scores.append((nll, i, candidate))


            option_scores.sort(key=lambda x: x[0])
            predicted_label = option_scores[0][1]
            is_correct = (predicted_label == correct_label)
            if is_correct:
                correct += 1


            if not is_correct:
                print(f"Błędna odpowiedź:\n  Kontekst: {context}\n  Opcje: {endings}\n  Poprawna etykieta: {correct_label}\n  Wybrana etykieta: {predicted_label}")


            results.append({
                "context": context,
                "endings": endings,
                "correct_label": correct_label,
                "predicted_label": predicted_label,
                "predicted_ending": endings[predicted_label],
                "correct": is_correct
            })

            if idx % 100 == 0:
                print(f"Przetworzono {idx + 1}/{total} przykładów. Aktualna dokładność: {(correct / (idx + 1)) * 100:.2f}%")

        except Exception as e:
            print(f"Błąd podczas przetwarzania przykładu {idx}: {e}")


    accuracy = correct / total if total > 0 else 0.0

    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump({
            "accuracy": accuracy,
            "results": results
        }, f, indent=2, ensure_ascii=False)

    print(f"HellaSwag accuracy: {accuracy * 100:.2f}%")
    return accuracy

######################################################
# 4. Test WinoGrande
######################################################

def test_winogrande(model, tokenizer, output_json_path="winogrande_results.json"):

    dataset = load_dataset("winogrande", "winogrande_xl")
    val_data = dataset["validation"]

    results = []
    correct = 0
    total = len(val_data)

    for example in val_data:

        sentence = example.get("sentence", "")
        option1 = example.get("option1", "")
        option2 = example.get("option2", "")
        answer_str = example.get("answer", None)


        if not sentence or not option1 or not option2 or not answer_str or "___" not in sentence:
            print(f"Pominięto przykład z brakującymi danymi: {example}")
            continue

        sentence_with_option1 = sentence.replace("___", option1)
        sentence_with_option2 = sentence.replace("___", option2)

        score1 = score_candidate(model, tokenizer, sentence_with_option1, "")
        score2 = score_candidate(model, tokenizer, sentence_with_option2, "")


        predicted_answer = "1" if score1 < score2 else "2"
        is_correct = (predicted_answer == answer_str)
        if is_correct:
            correct += 1


        results.append({
            "sentence": sentence,
            "option1": option1,
            "option2": option2,
            "correct_answer": answer_str,
            "predicted_answer": predicted_answer,
            "correct": is_correct
        })


    accuracy = correct / total if total > 0 else 0.0


    with open(output_json_path, "w", encoding="utf-8") as f:
        json.dump({
            "accuracy": accuracy,
            "results": results
        }, f, indent=2, ensure_ascii=False)

    print(f"Winogrande accuracy: {accuracy * 100:.2f}%")
    return accuracy


######################################################
# Główna część skryptu: uruchamianie wszystkich testów
######################################################
def main():

    model_name = "gpt2"  # Zmień tutaj na inny model, jeśli chcesz
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    model.eval()

    print("Testy rozpoczynają się...")

    acc_arc = test_arc_challenge(model, tokenizer, "arc_results.json")
    print(f"ARC-Challenge accuracy: {acc_arc * 100:.2f}%")

    acc_gsm8k = test_gsm8k(model, tokenizer, "gsm8k_results.json")
    print(f"GSM8K accuracy: {acc_gsm8k * 100:.2f}%")

    acc_hella = test_hellaswag(model, tokenizer, "hellaswag_results.json")
    print(f"HellaSwag accuracy (val): {acc_hella * 100:.2f}%")

    acc_wino = test_winogrande(model, tokenizer, "winogrande_results.json")
    print(f"WinoGrande accuracy (val): {acc_wino * 100:.2f}%")

    print("Wszystkie testy zakończone.")

if __name__ == "__main__":
    main()

